{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cds.climate.copernicus.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "unL9pphx_I_2"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikvC9fCrfZsA"
      },
      "source": [
        "!touch $HOME/.cdsapirc\n",
        "!echo \"url: https://cds.climate.copernicus.eu/api/v2\" >> $HOME/.cdsapirc\n",
        "!echo \"key: 71023:cf0744cc-00c0-47d2-b625-926642aa75e0\" >> $HOME/.cdsapirc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IE1vBWHnbTeH"
      },
      "source": [
        "!pip install "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9y5S6XtfoWF"
      },
      "source": [
        "!pip install cdsapi\n",
        "# !pip install cfgrib\n",
        "!apt install libeccodes-tools\n",
        "!pip install missingno"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sapOv3Ji7bI"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7IEMYNCE-XD"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plwt1895f_J5"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdwPhcr1f-2w"
      },
      "source": [
        "import cdsapi\n",
        "import xarray as xr\n",
        "import matplotlib.pyplot as plt\n",
        "import missingno as msno \n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import os\n",
        "import json\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import pdb\n",
        "import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# default `log_dir` is \"runs\" - we'll be more specific here\n",
        "writer = SummaryWriter()\n",
        "\n",
        "### We have to use daskt to handle such big dataset\n",
        "from dask.distributed import Client\n",
        "\n",
        "client = Client()\n",
        "client"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ktuh-K0g4nT3"
      },
      "source": [
        "if torch.cuda.is_available():  \n",
        "  dev = \"cuda:0\" \n",
        "else:  \n",
        "  dev = \"cpu\"  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whOJLpqy1jkB"
      },
      "source": [
        "# Download Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drYlsmwKHDLT"
      },
      "source": [
        "Let's download the data from the CDS API,\n",
        "\n",
        "Here I am only using 3 days of data because interpolating with anything bigger than it is exhausting my 12GB Ram limit on \"collab\". And \"dask\" does not support Interpolation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwoC_C4OqhyC"
      },
      "source": [
        "import cdsapi\n",
        "\n",
        "c = cdsapi.Client()\n",
        "\n",
        "c.retrieve(\n",
        "    'reanalysis-era5-land',\n",
        "    {\n",
        "        'format': 'netcdf',\n",
        "        'variable': [\n",
        "            '2m_temperature', 'total_precipitation', 'volumetric_soil_water_layer_1',\n",
        "        ],\n",
        "        'year': '2019',\n",
        "        'month': '12',\n",
        "        'time': '12:00',\n",
        "        'day': [\n",
        "              '01', '02','03',\n",
        "              # '04', '05', '06',\n",
        "              # '07', '08', '09',\n",
        "              # '10', '11', '12',\n",
        "              # '13', '14', '15',\n",
        "        ],\n",
        "    },\n",
        "    'download.nc')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfgWrveUheGH"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0CjHi85kQUE"
      },
      "source": [
        "dat = xr.open_dataset(\"download.nc\",\n",
        "    # chunks={\n",
        "    #     \"latitude\": 250,\n",
        "    #     \"longitude\": 250,\n",
        "    #     \"time\": -1,\n",
        "    # },  # this tells xarray to open the dataset as a dask array\n",
        ")\n",
        "with xr.set_options(display_style=\"text\"):\n",
        "  display(dat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q99IKi9nG3aX"
      },
      "source": [
        "# Read DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCfDOkTLHHsY"
      },
      "source": [
        "We are doing some extra calculations from getting a general Idea for our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vy86bQhgqq7"
      },
      "source": [
        "with xr.set_options(display_style=\"text\"):\n",
        "  lon,lat,t = dat.indexes.values()\n",
        "  print(f\"Lon Range: {lon[0]}, {lon[-1]}\")\n",
        "  print(f\"Lat Range: {lat[0]}, {lat[-1]}\")\n",
        "  print(f\"Time Range: {t[0]}, {t[-1]}\")\n",
        "  print(f\"Data values i.e t2m \\n min:{dat['t2m'].min().values}, max:{dat['t2m'].max().values}, count:{dat['t2m'].count().values}\")\n",
        "  print(f\"Data values i.e tp \\n min:{dat['tp'].min().values}, max:{dat['tp'].max().values}, count:{dat['tp'].count().values}\")\n",
        "  print(f\"Data values i.e swvl1 \\n min:{dat['swvl1'].min().values}, max:{dat['swvl1'].max().values}, count:{dat['swvl1'].count().values}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1swqeC4V5usU"
      },
      "source": [
        "## Till now we know\n",
        "~ It is a 3d data.\n",
        "\n",
        "~ Axis beign \n",
        "  * longitude  (longitude)  0.0, 359.8999938964844\n",
        "  * latitude   (latitude) 90.0, -90.0\n",
        "  * time       (time) 2019-12-01 12:00:00, 2019-12-15 12:00:00\n",
        "\n",
        "~ Then for each of our datasets we have our data values, ranging between.\n",
        "* Temperature goes from222.674560546875 to 315.7596740722656\n",
        "-> temperature goes from -15 to 41.85 C. Seems reasonable.\n",
        "\n",
        "* total_precipitation i.e t2m min:7.450580596923828e-09,max: 0.21889278292655945\n",
        "-> Such low values are reasonable as in most places it doesn't rain. But It is in meters let's convert it into millimeters\n",
        "\n",
        "* volumetric_soil_water_layer_1 i.e \n",
        "min:0.0, max: 0.7660064697265625\n",
        "-> It's valued is in m^3 m^(-3). So I don't think we should convert it. Because it is water per unit of volume.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e4Re99BF1az"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YghK6bkY9-z2"
      },
      "source": [
        "##  mean, median, standard dev., variance, range, spatial resolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbrdNLxjw4Vy"
      },
      "source": [
        "# Data Values from all the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM3CL5cJIbec"
      },
      "source": [
        "print(\n",
        "    \"\\tt2\\t\\ttp\\t\\tswvl1\\n\"\n",
        ")\n",
        "print(\"Mean\\t\",dat.mean(axis=(0,1,2)).to_array().values)\n",
        "print(\"Median\\t\",dat.median(axis=(0,1,2)).to_array().values)\n",
        "print(\"Std\\t\",dat.std(axis=(0,1,2)).to_array().values)\n",
        "print(\"Var\\t\",dat.var(axis=(0,1,2)).to_array().values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCl1mVbHeob9"
      },
      "source": [
        "Here we are calculating these values for the entirety of our data set. \n",
        "Herewith mean we are trying to see where our data lies within each sub-domain[\"t2\", ... etc]. So the temperature lies somewhere around 268 k and similarly precipitation mean is in 1e-4 and volumetric water is in 1e-1. We might need to normalize our data to make it easier for our neural network to learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QF5xOzoaxBt-"
      },
      "source": [
        "## Data Values on each data, i.e. averaging over latitude and longtitude."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpAcUnuqZfgN"
      },
      "source": [
        "dat[\"t2m\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOkgOoiXEb-d"
      },
      "source": [
        "fig, axes = plt.subplots(3, 3)\n",
        "fig.set_figheight(15)\n",
        "fig.set_figwidth(20)\n",
        "\n",
        "## MEAN\n",
        "axes[0][0].plot(dat[\"t2m\"].mean((\"latitude\", \"longitude\")))\n",
        "axes[0][0].set_title(label=\"2m_temperature_mean\")\n",
        "axes[1][0].plot(dat[\"swvl1\"].mean((\"latitude\", \"longitude\")))\n",
        "axes[1][0].set_title(label=\"volumetric_soil_water_layer_1_mean\")\n",
        "axes[2][0].plot(dat[\"tp\"].mean((\"latitude\", \"longitude\")))\n",
        "axes[2][0].set_title(label=\"total_precipitation_mean\")\n",
        "\n",
        "# MEDIAN\n",
        "axes[0][1].plot(dat[\"t2m\"].median((\"latitude\", \"longitude\")))\n",
        "axes[0][1].set_title(label=\"2m_temperature_median\")\n",
        "axes[1][1].plot(dat[\"swvl1\"].median((\"latitude\", \"longitude\")))\n",
        "axes[1][1].set_title(label=\"volumetric_soil_water_layer_1_median\")\n",
        "axes[2][1].plot(dat[\"tp\"].median((\"latitude\", \"longitude\")))\n",
        "axes[2][1].set_title(label=\"total_precipitation_median\")\n",
        "\n",
        "#STD\n",
        "axes[0][2].plot(dat[\"t2m\"].std((\"latitude\", \"longitude\")))\n",
        "axes[0][2].set_title(label=\"2m_temperature_std\")\n",
        "axes[1][2].plot(dat[\"swvl1\"].std((\"latitude\", \"longitude\")))\n",
        "axes[1][2].set_title(label=\"volumetric_soil_water_layer_1_std\")\n",
        "axes[2][2].plot(dat[\"tp\"].std((\"latitude\", \"longitude\")))\n",
        "axes[2][2].set_title(label=\"total_precipitation_std\")\n",
        "\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rso0_uCtK5sT"
      },
      "source": [
        "Here we can observer direct co-relation between tempreature,volumetric water and total precipitation.\n",
        "I am trying to mean over all of latitude and longitude to get a mean on all days of our dataset. \n",
        "Lower temprature give us more precipitaton, but sometime heigher temprature get's more humidity so the volumetric soil water is heigher which is directly proportional to precipitaton. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAtfpz7ALIxa"
      },
      "source": [
        "Plot a randomly chosen day from each of the datasets and use a sequential colormap for the plot. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IJD5BWhxXE8"
      },
      "source": [
        "fig, axes = plt.subplots(3)\n",
        "fig.set_figheight(15)\n",
        "fig.set_figwidth(20)\n",
        "dat[\"t2m\"].sel(time=\"2019-12-01\").plot(ax=axes[0])\n",
        "axes[0].set_title(label=\"2m_temperature_mean\")\n",
        "dat[\"swvl1\"].sel(time=\"2019-12-01\").plot(ax=axes[1])\n",
        "axes[1].set_title(label=\"volumetric_soil_water_layer_1_mean\")\n",
        "dat[\"tp\"].sel(time=\"2019-12-01\").plot(ax=axes[2]) ## Having trouble because of highly skewed data.\n",
        "axes[2].set_title(label=\"total_precipitation_mean\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Wrtm1WNkmei"
      },
      "source": [
        "Places like Africa have more temperature and less volumetric water and vice-versa. But our total precipitation is awfully blank. Let's investigate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o51LGkW4zJYc"
      },
      "source": [
        "fig, axes = plt.subplots(3)\n",
        "\n",
        "dat[\"t2m\"].plot(ax=axes[0]) ## Having trouble because of highly skewed data.\n",
        "axes[0].set_title(label=\"2m_temperature\")\n",
        "\n",
        "dat[\"swvl1\"].plot(ax=axes[1]) ## Having trouble because of highly skewed data.\n",
        "axes[1].set_title(label=\"volumetric_soil_water_layer_1\")\n",
        "\n",
        "dat[\"tp\"].plot(ax=axes[2]) ## Having trouble because of highly skewed data.\n",
        "axes[2].set_title(label=\"total_precipitation\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHXcWnnjz-kj"
      },
      "source": [
        "Here our problem is much more clear. We have left-skewed data in total precipitation. It's is expected though, since most places do not have that much precipitation and few places will have much more precipitation. We need to do change this data to make it easy for our neural network and not to get stuck in local minima of 0.0 value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXDvNWDTnAWV"
      },
      "source": [
        "## Solving Skewd Total Precipitation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWs9QGWbnfIz"
      },
      "source": [
        "To solve left-skewed data we can do many things. \n",
        "\n",
        "The main one that works are. \n",
        "* Taking the log of every value in the dataset\n",
        "* Taking a box cox of the dataset from scipy.stats package\n",
        "\n",
        "Other methods include using min-max reduction, cube root, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufwjdfzrs4UQ"
      },
      "source": [
        "box_cox_values = stats.boxcox(dat[\"tp\"].values.ravel())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZRczm3ppBhj"
      },
      "source": [
        "plt.hist(box_cox_values, bins=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfBYXv3wpF_e"
      },
      "source": [
        "plt.hist(dat[\"tp\"].values.ravel(), bins=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y27n-yyNplTf"
      },
      "source": [
        "plt.hist(np.log(dat[\"tp\"].values.ravel()), bins=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrpxDZC-tU1L"
      },
      "source": [
        "dat[\"tp\"] = np.log(dat[\"tp\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyLDbUbupvZE"
      },
      "source": [
        "Since taking a log every value is much faster than boxcox. I am going to use np.log. Also logging give us much more varied and spread data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHmUTuqsjZ88"
      },
      "source": [
        "## Change Spatial Resolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2GnOZwT6AEQ"
      },
      "source": [
        "dat = dat.interp(\n",
        "    longitude=np.arange(90, -90, -0.05),\n",
        "    latitude=np.arange(0, 360, 0.05),\n",
        ")\n",
        "# Lon Range: 0.0, 359.8999938964844\n",
        "# Lat Range: 90.0, -90.0\n",
        "# Time Range: 2019-12-01 12:00:00, 2019-12-15 12:00:00\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXR9GPxM_FTu"
      },
      "source": [
        "dat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_KtWWgC71wR"
      },
      "source": [
        "fig, axes = plt.subplots(3)\n",
        "\n",
        "dat[\"t2m\"].plot(ax=axes[0]) ## Having trouble because of highly skewed data.\n",
        "axes[0].set_title(label=\"2m_temperature_mean\")\n",
        "\n",
        "dat[\"swvl1\"].plot(ax=axes[1]) ## Having trouble because of highly skewed data.\n",
        "axes[1].set_title(label=\"volumetric_soil_water_layer_1_mean\")\n",
        "\n",
        "dat[\"tp\"].plot(ax=axes[2]) ## Having trouble because of highly skewed data.\n",
        "axes[2].set_title(label=\"total_precipitation_mean\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeB-Ak038NL7"
      },
      "source": [
        "fig, axes = plt.subplots(3, 3)\n",
        "fig.set_figheight(15)\n",
        "fig.set_figwidth(20)\n",
        "\n",
        "## MEAN\n",
        "axes[0][0].plot(dat[\"t2m\"].mean((\"latitude\", \"longitude\")))\n",
        "axes[0][0].set_title(label=\"2m_temperature_mean\")\n",
        "axes[1][0].plot(dat[\"swvl1\"].mean((\"latitude\", \"longitude\")))\n",
        "axes[1][0].set_title(label=\"volumetric_soil_water_layer_1_mean\")\n",
        "axes[2][0].plot(dat[\"tp\"].mean((\"latitude\", \"longitude\")))\n",
        "axes[2][0].set_title(label=\"total_precipitation_mean\")\n",
        "\n",
        "# MEDIAN\n",
        "axes[0][1].plot(dat[\"t2m\"].median((\"latitude\", \"longitude\")))\n",
        "axes[0][1].set_title(label=\"2m_temperature_median\")\n",
        "axes[1][1].plot(dat[\"swvl1\"].median((\"latitude\", \"longitude\")))\n",
        "axes[1][1].set_title(label=\"volumetric_soil_water_layer_1_median\")\n",
        "axes[2][1].plot(dat[\"tp\"].median((\"latitude\", \"longitude\")))\n",
        "axes[2][1].set_title(label=\"total_precipitation_median\")\n",
        "\n",
        "#STD\n",
        "axes[0][2].plot(dat[\"t2m\"].std((\"latitude\", \"longitude\")))\n",
        "axes[0][2].set_title(label=\"2m_temperature_std\")\n",
        "axes[1][2].plot(dat[\"swvl1\"].std((\"latitude\", \"longitude\")))\n",
        "axes[1][2].set_title(label=\"volumetric_soil_water_layer_1_std\")\n",
        "axes[2][2].plot(dat[\"tp\"].std((\"latitude\", \"longitude\")))\n",
        "axes[2][2].set_title(label=\"total_precipitation_std\")\n",
        "\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R8kjQ8jUvT5"
      },
      "source": [
        "## Handle NAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbdWfgjUx1wW"
      },
      "source": [
        "Some more things to do here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEKaNo-erIA6"
      },
      "source": [
        "Since we are trying to predict precipitaion values as our task, so nan values for precipitaion is does not give use any information and changing them with mean values is also useless.\n",
        "So we drop all values that have total precipitaton as nan and change for another."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JvA-J4TClHa"
      },
      "source": [
        "dat[\"tp\"].sel(time=\"2019-12-01\").isnull().sum(dim=\"latitude\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSbnN3T6E7_h"
      },
      "source": [
        "dat[\"tp\"].sel(time=\"2019-12-01\").isnull().sum(dim=\"longitude\").plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilBhmnI5FOMm"
      },
      "source": [
        "dat[\"tp\"].sel(time=\"2019-12-01\").isnull().sum(dim=\"latitude\").plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYbwMxkOsOT-"
      },
      "source": [
        "For some values of latitude we have almost all nan values make sence since the values in pacific ocean or north pole is not going to be available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiS6o3AdHb-e"
      },
      "source": [
        "dat[\"t2m\"].isnull()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik0PdmqQUyLJ"
      },
      "source": [
        "rn removing all nan values for clearity... and converting them to dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM4lyKcHUxzH"
      },
      "source": [
        "dat = dat.to_dataframe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL0G64yFIy_Q"
      },
      "source": [
        "dat.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQamtI1rzw_N"
      },
      "source": [
        "dat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXRx2CsQOTqA"
      },
      "source": [
        "##Deep Learning for Geospatial Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iet8e4RNNCSS"
      },
      "source": [
        "Since there are no categorical data embedding layer will not help us here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mugAuaUPAjHM"
      },
      "source": [
        "* ## 1.)  How you will split the data for training, validation & testing? \n",
        "-> I will be using scikit. learn train_test_split function to split the data in train and test function. If I ever need a validation dataset I will further split the dataset. I can also apply K-fold on every this dataset. \n",
        "* ## 2.)  Implementations for data loading, data transformations & inverse transformation\n",
        "-> I normalized the input dataset, I saved mean and std in a dictionary to normalize in evaluation.\n",
        "I will use \"np. exp\" to get my result. Because of np. exp is the value of np.log.\n",
        "* ## 3.) Choice of Model architecture\n",
        "I am using a very simple model because we don't have many features to play with here. But I added a residual connection to help in any linear relations\n",
        "* ## 4.) Obtaining and Fine-tuning a pre-trained model if used\n",
        "I used my own network, so I don't need to fine-tune any model.\n",
        "* ## 5.) Function descriptions and definitions for model training, testing, and inference. \n",
        "-> I trained and tested in the same loop, but created a separate file that can be used to inference on it's alone.\n",
        "* ## 6.)Implementation techniques that help improve the efficiency of model training and data loading. \n",
        "General methods are Normalization, Standardization, adding residual connections.\n",
        "To increase the efficiency we can load our data in the GPU using .to(Cuda)\n",
        "* ## 7.)How you will avoid overfitting (and implementation of solution). \n",
        "-> I added dropouts layers. And I also implemented batch norm, in some cases, it helps to apply batch norm.\n",
        "* ## 8.) What do you use for visualizing model training and performance? \n",
        "I use tensorboard here. But in personal projects I also use WandB.\n",
        "* ## 9.) What factors do you think might restrict the model from achieving a high accuracy?\n",
        "We don't have many features to work with. Our data is too skewed. We need to apply more EDA to extract better features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzS4oAB52Cfy"
      },
      "source": [
        "## Standardise input to the neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3u_UDQ0w2mth"
      },
      "source": [
        "## Normalize data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulA6C43mhe__"
      },
      "source": [
        "dat.to_csv(\"name.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT5W_5KViDeQ"
      },
      "source": [
        "dat = pd.read_csv(\"name.csv\", index_col=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ORwUOEes9QY"
      },
      "source": [
        "dat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHwu2ezgx6h7"
      },
      "source": [
        "Here I am using latitude and longitude in the model because the location of data point might help in our prediction. I normalized that too becuase it gave me much better result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBHWYQwoFhDK"
      },
      "source": [
        "t2m_mean = dat[\"t2m\"].mean()\n",
        "t2m_std  = dat[\"t2m\"].std()\n",
        "\n",
        "swvl1_std  = dat[\"swvl1\"].mean()\n",
        "swvl1_std  = dat[\"swvl1\"].std()\n",
        "\n",
        "latitude_std  = dat[\"latitude\"].mean()\n",
        "latitude_std  = dat[\"latitude\"].std()\n",
        "\n",
        "longitude_std  = dat[\"longitude\"].mean()\n",
        "longitude_std  = dat[\"longitude\"].std()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sDFgUzEF1Ae"
      },
      "source": [
        "re_normalize_vals ={\n",
        "    \"mean\":{\n",
        "        \"t2m\":dat[\"t2m\"].mean(),\n",
        "        \"swvl1\":dat[\"swvl1\"].mean(),\n",
        "        \"latitude\":dat[\"latitude\"].mean(),\n",
        "        \"longitude\": dat[\"longitude\"].mean()\n",
        "    },\n",
        "    \"std\":{\n",
        "        \"t2m\":dat[\"t2m\"].std(),\n",
        "        \"swvl1\":dat[\"swvl1\"].std(),\n",
        "        \"latitude\":dat[\"latitude\"].std(),\n",
        "        \"longitude\": dat[\"longitude\"].std()\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSpmd-09GY0s"
      },
      "source": [
        "re_normalize_vals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuRr1yXENKGd"
      },
      "source": [
        "## Transformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L78UQe302loP"
      },
      "source": [
        "## Normalze data\n",
        "dat[\"t2m\"] = (dat[\"t2m\"] - dat[\"t2m\"].mean())/(1e-7  + dat[\"t2m\"].std())\n",
        "dat[\"swvl1\"] = (dat[\"swvl1\"] - dat[\"swvl1\"].mean())/(1e-7  + dat[\"swvl1\"].std())\n",
        "dat[\"latitude\"] = (dat[\"latitude\"] - dat[\"latitude\"].mean())/(1e-7  + dat[\"latitude\"].std())\n",
        "dat[\"longitude\"] = (dat[\"longitude\"] - dat[\"longitude\"].mean())/(1e-7  + dat[\"longitude\"].std())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yx3B2rAjyW2Z"
      },
      "source": [
        "dat = dat.drop([\"time\"], axis=1)\n",
        "dat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ9AQxGnyuR8"
      },
      "source": [
        "Y = dat[\"tp\"].to_numpy()\n",
        "dat = dat.drop([\"tp\"], axis=1).to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af-mh-HM0fGM"
      },
      "source": [
        "### Let's split the dataset. I will use 33% of data to check our values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T714o9_-zhGX"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(dat, Y, test_size=0.33, random_state=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DugzrfLzbKs"
      },
      "source": [
        "x_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzgC_bSU1ze5"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5lfesLpAmoF"
      },
      "source": [
        "Implementations for data loading, data transformations & \n",
        "\n",
        "---> inverse transformation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEb0NweG14oa"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7aT1FmCtgu2"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztNXE8ZoOUvj"
      },
      "source": [
        "class GeoDataset(Dataset):\n",
        "  def __init__(self, X, y, transforms=None):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "  def __len__(self):\n",
        "    return len(self.y)\n",
        "  def __getitem__(self, idx):\n",
        "    return torch.tensor([self.X[idx][0], self.X[idx][1], self.X[idx][2], self.X[idx][3]]), torch.tensor([self.y[idx]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rga8z_i7Prwl"
      },
      "source": [
        "train_dataset = GeoDataset(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OasQkKh4Muw"
      },
      "source": [
        "test_dataset = GeoDataset(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rduCEDOnS4CV"
      },
      "source": [
        "for dat in train_dataset:\n",
        "  print(dat)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObIcZVib3LPU"
      },
      "source": [
        "len(train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPiSaRO4P38f"
      },
      "source": [
        "train_dl = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
        "test_dl = DataLoader(test_dataset, batch_size=256, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w28QmP7sPzIM"
      },
      "source": [
        "for data in train_dl:\n",
        "  print(data)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3uLP4p6QTo9"
      },
      "source": [
        "#### Create Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKlp1gsQtnOu"
      },
      "source": [
        "We are going to use dropout to prevent from overfitting. Becuase we can have as much data as much we want and this problem is not that difficult our simple model can approximate it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUk7aDxlP1Nc"
      },
      "source": [
        "class GeoModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.lin1 = nn.Linear(4, 4)\n",
        "    self.drop = nn.Dropout(0.2)\n",
        "    self.lin2 = nn.Linear(4, 8)\n",
        "\n",
        "    self.lin3 = nn.Linear(8, 4)\n",
        "    self.bn1  = nn.BatchNorm1d(4)\n",
        "\n",
        "    self.lin4 = nn.Linear(4, 2)\n",
        "    self.lin5 = nn.Linear(2, 1)\n",
        "\n",
        "  def forward(self, X):\n",
        "    x_temp = X\n",
        "    x = F.relu(self.lin1(X))\n",
        "    x = F.relu(self.lin2(x))\n",
        "    x = self.drop(x)\n",
        "    x = F.relu(self.lin3(x) + x_temp) # I added a resedual connection so if any linear pattern exists\n",
        "    x = self.bn1(x)\n",
        "    x = F.relu(self.lin4(x))\n",
        "\n",
        "    x = self.lin5(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCRWT7hjWpWV"
      },
      "source": [
        "net = GeoModel().to(dev)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIqkcnEcRAYZ"
      },
      "source": [
        "criterion = nn.SmoothL1Loss()\n",
        "optimizer = optim.Adam(net.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ebMMfvywRMG"
      },
      "source": [
        "criterion(torch.randn(100, 128), torch.randn(100, 128)).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbNy4OBEwy1v"
      },
      "source": [
        "input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo9OgJGzgFiU"
      },
      "source": [
        "total_loss   = 0.0\n",
        "for i, data in enumerate(train_dl):\n",
        "    input, label = data\n",
        "    out = net(input.float().to(dev))\n",
        "    print(out, label)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XgwbeR_Wq6M"
      },
      "source": [
        "def train(train_dataloader, test_data_loader, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    net.train()\n",
        "    running_loss = 0.0\n",
        "    total_loss   = 0.0\n",
        "    for i, data in enumerate(train_dataloader):\n",
        "      input, label = data\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      out = net(input.float().to(dev))\n",
        "      loss = criterion(out, label.float().to(dev))\n",
        "    #   print(out[:])\n",
        "    #   print(label)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item()\n",
        "      total_loss   += loss.item()\n",
        "      # break\n",
        "      if i%100==99:\n",
        "        writer.add_scalar('loss/train_running', running_loss/100, i)\n",
        "        # tf.summary.scalar('loss', train_loss.result(), step=i)\n",
        "        print(running_loss/100, epoch, i, \"train\")\n",
        "        running_loss = 0.0\n",
        "\n",
        "    writer.add_scalar('loss/train_total', total_loss/len(train_dataloader), epoch)\n",
        "    total_loss   = 0.0\n",
        "    running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        net.eval()\n",
        "        for i, data in enumerate(test_data_loader):\n",
        "            data, labels = data\n",
        "            outputs = net(data.float().to(dev))\n",
        "            loss = criterion(out, label.float().to(dev))\n",
        "            running_loss += loss.item()\n",
        "            total_loss   += loss.item()\n",
        "\n",
        "            # break\n",
        "            if i%100==0:\n",
        "              writer.add_scalar('loss/valid_running', running_loss, i)\n",
        "\n",
        "            #   tf.summary.scalar('loss', train_loss.result(), step=i)\n",
        "              print(running_loss/100, epoch, i, \"val\")\n",
        "              running_loss = 0.0\n",
        "        writer.add_scalar('loss/valid_total', total_loss/len(train_dataloader), epoch)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rajW59IRXfr8"
      },
      "source": [
        "train(train_dl, test_dl ,5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InxHUa0NDxiv"
      },
      "source": [
        "torch.save(net, \"weights.pth\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYVpfYKcxSx8"
      },
      "source": [
        "# I am going to use tesnsorboard to visualize my data output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5Wo6joi2aF_"
      },
      "source": [
        "%tensorboard --logdir runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0akRHpXDuAy"
      },
      "source": [
        "## EVAL Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dI3R_MrGeJ8"
      },
      "source": [
        "re_normalize_vals = {'mean': {'latitude': 38.40959136588079,\n",
        "  'longitude': 45.36507742195719,\n",
        "  'swvl1': 0.22580819576544764,\n",
        "    't2m': 281.8924812033501},\n",
        "    'std': {'latitude': 19.317098089735516,\n",
        "  'longitude': 25.47684871130826,\n",
        "  'swvl1': 0.15814413872235586,\n",
        "  't2m': 16.144941173993605}}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ4u2Rbt_E52"
      },
      "source": [
        "def normalizeForInput(latitude, longitude, t2m, swvl1):\n",
        "    latitude = (latitude - re_normalize_vals[\"mean\"][\"latitude\"])/(1e-7  + re_normalize_vals[\"std\"][\"latitude\"])\n",
        "    longitude = (longitude - re_normalize_vals[\"mean\"][\"longitude\"])/(1e-7  + re_normalize_vals[\"std\"][\"longitude\"])\n",
        "    t2m = (t2m - re_normalize_vals[\"mean\"][\"t2m\"])/(1e-7  + re_normalize_vals[\"std\"][\"t2m\"])\n",
        "    swvl1 = (swvl1 - re_normalize_vals[\"mean\"][\"swvl1\"])/(1e-7  + re_normalize_vals[\"std\"][\"swvl1\"])\n",
        "    return latitude, longitude, t2m, swvl1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVewhNV3HiQj"
      },
      "source": [
        "net_eval = torch.load(\"weights.pth\")\n",
        "\n",
        "# net_eval.load_state_dict(torch.load(\"weights.pth\")) \n",
        "net_eval.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o-nK8Z_H08M"
      },
      "source": [
        "out = net_eval(torch.tensor([[-85.0, 256.0, 1.334842,\t0.344491]]).to(dev))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8NZhCgTIk9F"
      },
      "source": [
        "def denoramlize_output(output):\n",
        "    return np.exp(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M3rIrXiI0Cn"
      },
      "source": [
        "denoramlize_output(out.detach().cpu())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8y_AvNVrJJy0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}